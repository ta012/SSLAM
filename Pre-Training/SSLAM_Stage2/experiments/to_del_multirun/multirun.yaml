hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: /mnt/fast/nobackup/users/ta01123//eat_apr30/fairseq//EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls/experiments/to_del_multirun
    subdir: ${hydra.job.num}
  hydra_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][HYDRA] %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
    root:
      level: INFO
      handlers:
      - console
    loggers:
      logging_example:
        level: DEBUG
    disable_existing_loggers: false
  job_logging:
    version: 1
    formatters:
      simple:
        format: '[%(asctime)s][%(name)s][%(levelname)s] - %(message)s'
    handlers:
      console:
        class: logging.StreamHandler
        formatter: simple
        stream: ext://sys.stdout
      file:
        class: logging.FileHandler
        formatter: simple
        filename: ${hydra.job.name}.log
    root:
      level: INFO
      handlers:
      - console
      - file
    disable_existing_loggers: false
  sweeper:
    _target_: hydra._internal.core_plugins.basic_sweeper.BasicSweeper
    max_batch_size: null
  launcher:
    _target_: hydra._internal.core_plugins.basic_launcher.BasicLauncher
  help:
    app_name: ${hydra.job.name}
    header: '${hydra.help.app_name} is powered by Hydra.

      '
    footer: 'Powered by Hydra (https://hydra.cc)

      Use --hydra-help to view Hydra specific help

      '
    template: '${hydra.help.header}

      == Configuration groups ==

      Compose your configuration from those groups (group=option)


      $APP_CONFIG_GROUPS


      == Config ==

      Override anything in the config (foo.bar=value)


      $CONFIG


      ${hydra.help.footer}

      '
  hydra_help:
    hydra_help: ???
    template: 'Hydra (${hydra.runtime.version})

      See https://hydra.cc for more info.


      == Flags ==

      $FLAGS_HELP


      == Configuration groups ==

      Compose your configuration from those groups (For example, append hydra/job_logging=disabled
      to command line)


      $HYDRA_CONFIG_GROUPS


      Use ''--cfg hydra'' to Show the Hydra config.

      '
  output_subdir: .hydra
  overrides:
    hydra:
    - hydra.sweep.dir=/mnt/fast/nobackup/users/ta01123//eat_apr30/fairseq//EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls/experiments/to_del_multirun
    task:
    - common.user_dir=EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls
    - optimization.max_update=200000
    - optimization.lr=[0.00005]
    - optimizer.groups.default.lr_float=0.00005
    - optimizer.groups.default.lr_scheduler.warmup_updates=25000
    - checkpoint.save_dir=/mnt/fast/nobackup/users/ta01123//eat_apr30/fairseq//EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls/experiments/to_del
    - checkpoint.continue_once=/mnt/fast/nobackup/users/ta01123/eat_ckpt/EAT-base_epoch10_pt.pt
    - checkpoint.reset_dataloader=True
    - checkpoint.reset_lr_scheduler=True
    - checkpoint.reset_meters=True
    - checkpoint.reset_optimizer=True
    - checkpoint.save_interval_updates=40262
    - checkpoint.keep_interval_updates=8
    - distributed_training.distributed_world_size=1
    - dataset.batch_size=12
    - task.data=/mnt/fast/nobackup/users/ta01123//eat_data_manifest/manifest_as2mfull_pre/
    - model.clone_batch=8
    - task.h5_format=False
  job:
    name: hydra_train
    override_dirname: checkpoint.continue_once=/mnt/fast/nobackup/users/ta01123/eat_ckpt/EAT-base_epoch10_pt.pt,checkpoint.keep_interval_updates=8,checkpoint.reset_dataloader=True,checkpoint.reset_lr_scheduler=True,checkpoint.reset_meters=True,checkpoint.reset_optimizer=True,checkpoint.save_dir=/mnt/fast/nobackup/users/ta01123//eat_apr30/fairseq//EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls/experiments/to_del,checkpoint.save_interval_updates=40262,common.user_dir=EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls,dataset.batch_size=12,distributed_training.distributed_world_size=1,model.clone_batch=8,optimization.lr=[0.00005],optimization.max_update=200000,optimizer.groups.default.lr_float=0.00005,optimizer.groups.default.lr_scheduler.warmup_updates=25000,task.data=/mnt/fast/nobackup/users/ta01123//eat_data_manifest/manifest_as2mfull_pre/,task.h5_format=False
    id: ???
    num: ???
    config_name: pretraining_AS2M
    env_set: {}
    env_copy: []
    config:
      override_dirname:
        kv_sep: '='
        item_sep: ','
        exclude_keys: []
  runtime:
    version: 1.0.7
    cwd: /mnt/fast/nobackup/users/ta01123/eat_apr30/fairseq/EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixClsWAV/scripts
  verbose: false
_name: null
common:
  _name: null
  no_progress_bar: false
  log_interval: 200
  log_format: json
  log_file: null
  aim_repo: null
  aim_run_hash: null
  tensorboard_logdir: tb
  wandb_project: null
  azureml_logging: false
  seed: 1
  cpu: false
  tpu: false
  bf16: false
  memory_efficient_bf16: false
  fp16: true
  memory_efficient_fp16: false
  fp16_no_flatten_grads: true
  fp16_init_scale: 128
  fp16_scale_window: null
  fp16_scale_tolerance: 0.0
  on_cpu_convert_precision: false
  min_loss_scale: 1.0e-06
  threshold_loss_scale: null
  amp: false
  amp_batch_retries: 2
  amp_init_scale: 128
  amp_scale_window: null
  user_dir: EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls
  empty_cache_freq: 0
  all_gather_list_size: 16384
  model_parallel_size: 1
  quantization_config_path: null
  profile: false
  reset_logging: false
  suppress_crashes: false
  use_plasma_view: false
  plasma_path: /tmp/plasma
common_eval:
  _name: null
  path: null
  post_process: null
  quiet: false
  model_overrides: '{}'
  results_path: null
distributed_training:
  _name: null
  distributed_world_size: 1
  distributed_num_procs: 1
  distributed_rank: 0
  distributed_backend: nccl
  distributed_init_method: null
  distributed_port: -1
  device_id: 0
  distributed_no_spawn: false
  ddp_backend: c10d
  ddp_comm_hook: none
  bucket_cap_mb: 25
  fix_batches_to_gpus: false
  find_unused_parameters: false
  gradient_as_bucket_view: false
  fast_stat_sync: false
  heartbeat_timeout: -1
  broadcast_buffers: false
  slowmo_momentum: null
  slowmo_base_algorithm: localsgd
  localsgd_frequency: 3
  nprocs_per_node: 1
  pipeline_model_parallel: false
  pipeline_balance: null
  pipeline_devices: null
  pipeline_chunks: 0
  pipeline_encoder_balance: null
  pipeline_encoder_devices: null
  pipeline_decoder_balance: null
  pipeline_decoder_devices: null
  pipeline_checkpoint: never
  zero_sharding: none
  fp16: ${common.fp16}
  memory_efficient_fp16: ${common.memory_efficient_fp16}
  tpu: ${common.tpu}
  no_reshard_after_forward: false
  fp32_reduce_scatter: false
  cpu_offload: false
  use_sharded_state: false
  not_fsdp_flatten_parameters: false
dataset:
  _name: null
  num_workers: 10
  skip_invalid_size_inputs_valid_test: true
  max_tokens: null
  batch_size: 12
  required_batch_size_multiple: 1
  required_seq_len_multiple: 1
  dataset_impl: null
  data_buffer_size: 10
  train_subset: train
  valid_subset: valid
  combine_valid_subsets: null
  ignore_unused_valid_subsets: false
  validate_interval: 1
  validate_interval_updates: 0
  validate_after_updates: 0
  fixed_validation_seed: null
  disable_validation: true
  max_tokens_valid: ${dataset.max_tokens}
  batch_size_valid: ${dataset.batch_size}
  max_valid_steps: null
  curriculum: 0
  gen_subset: test
  num_shards: 1
  shard_id: 0
  grouped_shuffling: false
  update_epoch_batch_itr: ${dataset.grouped_shuffling}
  update_ordered_indices_seed: false
optimization:
  _name: null
  max_epoch: 0
  max_update: 200000
  stop_time_hours: 0.0
  clip_norm: 4.0
  sentence_avg: false
  update_freq:
  - 1
  lr:
  - 5.0e-05
  stop_min_lr: -1.0
  use_bmuf: false
  skip_remainder_batch: false
  debug_param_names: true
checkpoint:
  _name: null
  save_dir: /mnt/fast/nobackup/users/ta01123//eat_apr30/fairseq//EAT_CkptMixRegionExtraMixLCls1Layer2S2TDrpInpTBothNoMixCls/experiments/to_del
  restore_file: checkpoint_last.pt
  continue_once: /mnt/fast/nobackup/users/ta01123/eat_ckpt/EAT-base_epoch10_pt.pt
  finetune_from_model: null
  reset_dataloader: true
  reset_lr_scheduler: true
  reset_meters: true
  reset_optimizer: true
  optimizer_overrides: '{}'
  save_interval: 1
  save_interval_updates: 40262
  keep_interval_updates: 8
  keep_interval_updates_pattern: -1
  keep_last_epochs: -1
  keep_best_checkpoints: -1
  no_save: false
  no_epoch_checkpoints: true
  no_last_checkpoints: false
  no_save_optimizer_state: false
  best_checkpoint_metric: loss
  maximize_best_checkpoint_metric: false
  patience: -1
  checkpoint_suffix: ''
  checkpoint_shard_count: 1
  load_checkpoint_on_all_dp_ranks: false
  write_checkpoints_asynchronously: false
  model_parallel_size: ${common.model_parallel_size}
bmuf:
  _name: null
  block_lr: 1.0
  block_momentum: 0.875
  global_sync_iter: 50
  warmup_iterations: 500
  use_nbm: false
  average_sync: false
  distributed_world_size: ${distributed_training.distributed_world_size}
generation:
  _name: null
  beam: 5
  beam_mt: 0
  nbest: 1
  max_len_a: 0.0
  max_len_b: 200
  max_len_a_mt: 0.0
  max_len_b_mt: 200
  min_len: 1
  match_source_len: false
  unnormalized: false
  no_early_stop: false
  no_beamable_mm: false
  lenpen: 1.0
  lenpen_mt: 1.0
  unkpen: 0.0
  replace_unk: null
  sacrebleu: false
  score_reference: false
  prefix_size: 0
  no_repeat_ngram_size: 0
  sampling: false
  sampling_topk: -1
  sampling_topp: -1.0
  constraints: null
  temperature: 1.0
  diverse_beam_groups: -1
  diverse_beam_strength: 0.5
  diversity_rate: -1.0
  print_alignment: null
  print_step: false
  lm_path: null
  lm_weight: 0.0
  iter_decode_eos_penalty: 0.0
  iter_decode_max_iter: 10
  iter_decode_force_max_iter: false
  iter_decode_with_beam: 1
  iter_decode_with_external_reranker: false
  retain_iter_history: false
  retain_dropout: false
  retain_dropout_modules: null
  decoding_format: null
  no_seed_provided: false
  eos_token: null
eval_lm:
  _name: null
  output_word_probs: false
  output_word_stats: false
  context_window: 0
  softmax_batch: 9223372036854775807
interactive:
  _name: null
  buffer_size: 0
  input: '-'
model:
  _name: data2vec_multi
  ema_decay: 0.9998
  ema_end_decay: 0.99999
  ema_anneal_end_step: 100000
  instance_norm_target_layer: true
  layer_norm_target_layer: false
  layer_norm_targets: true
  end_of_block_targets: false
  depth: 12
  average_top_k_layers: 12
  clone_batch: 8
  norm_eps: 1.0e-06
  min_target_var: 0
  min_pred_var: 0
  encoder_dropout: 0
  post_mlp_drop: 0
  attention_dropout: 0
  activation_dropout: 0
  supported_modality: IMAGE
  cls_loss: 1
  ema_encoder_only: false
  modalities:
    image:
      in_chans: 1
      inverse_mask: true
      mask_prob: 0.8
      mask_prob_adjust: 0.07
      mask_length: 5
      mask_noise_std: 0.01
      prenet_depth: 0
      ema_local_encoder: true
      num_extra_tokens: 1
      init_extra_token_zero: false
      use_alibi_encoder: false
      decoder:
        decoder_dim: 768
        decoder_groups: 16
        decoder_kernel: 3
        decoder_layers: 6
        input_dropout: 0
task:
  _name: mae_image_pretraining
  data: /mnt/fast/nobackup/users/ta01123//eat_data_manifest/manifest_as2mfull_pre/
  rebuild_batches: true
  key: source
  precompute_mask_config: {}
  downsr_16hz: true
  audio_mae: true
  h5_format: false
  target_length: 1024
  flexible_mask: false
criterion:
  _name: model
  log_keys:
  - ema_decay
  - target_var
  - pred_var
  - model_norm
  - ema_norm
  - masked_pct
optimizer:
  _name: composite
  dynamic_groups: true
  groups:
    default:
      lr_float: 5.0e-05
      optimizer:
        _name: adam
        adam_betas:
        - 0.9
        - 0.95
        weight_decay: 0.05
      lr_scheduler:
        _name: cosine
        warmup_updates: 25000
lr_scheduler: pass_through
scoring: null
bpe: null
tokenizer: null
ema:
  _name: null
  store_ema: false
  ema_decay: 0.9999
  ema_start_update: 0
  ema_seed_model: null
  ema_update_freq: 1
  ema_fp32: false
